{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "import model_names\n",
    "\n",
    "# Load the model names\n",
    "all_data = model_names.model_names\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_data = [tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors='pt') for text in all_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensors\n",
    "input_ids = torch.cat([d['input_ids'] for d in tokenized_data], dim=0)\n",
    "attention_masks = torch.cat([d['attention_mask'] for d in tokenized_data], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels (0 for all data)\n",
    "labels = torch.zeros(len(all_data))\n",
    "\n",
    "# Split data into train and test\n",
    "train_inputs, test_inputs, train_labels, test_labels = input_ids, input_ids, labels, labels\n",
    "train_masks, test_masks = attention_masks, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoader\n",
    "batch_size = 4\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)  # We're doing binary classification, so num_labels=1\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 777/777 [31:03<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.002400513429053215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 777/777 [27:46<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0006748705350412233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 777/777 [39:22<00:00,  3.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0003054956997014156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Epoch %d\" % epoch):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2].unsqueeze(1)}  # Unsqueezing labels to match shape\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Average training loss:\", total_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sklearn' is not classified as a model/library. Prediction: 0.49942097067832947\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        keyword = \"sklearn\"  # Example keyword to classify\n",
    "        inputs = tokenizer.encode_plus(keyword, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_tensors='pt').to(device)\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        prediction = torch.sigmoid(outputs.logits).item()\n",
    "        if prediction > 0.5:\n",
    "            print(f\"'{keyword}' is classified as a model/library. Prediction: {prediction}\")\n",
    "        else:\n",
    "            print(f\"'{keyword}' is not classified as a model/library. Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# torch.save(model.state_dict(), \"keyword_extraction_model.pth\")\n",
    "\n",
    "# # Export tokenizer to pickle file\n",
    "# import pickle\n",
    "# with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f339c050ae040bbbd17d7d752a8d518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant Kesharia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Vedant Kesharia\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa8378ea4014fa89d1443c36e062206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f174198ba3247b7a4ce88ca80918554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e832bd316ea474297a556a81d11a3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6db718d6ab403d849319295fba9f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83c59cdca3b4273bffc92a4fac136a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant Kesharia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 195/195 [04:29<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0021020583918867396\n",
      "'scikit-learn' is classified as a model/library.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 195/195 [04:58<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0015899744751946762\n",
      "'scikit-learn' is not classified as a model/library. Prediction: 0.49545249342918396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 195/195 [04:53<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0010678176280821507\n",
      "'scikit-learn' is classified as a model/library.\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "# from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "# from tqdm import tqdm\n",
    "# import model_names\n",
    "\n",
    "# # Load the model names\n",
    "# all_data = model_names.model_names\n",
    "\n",
    "# # Create labels (0 for all data)\n",
    "# labels = torch.zeros(len(all_data))\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
    "\n",
    "# # Tokenize the data\n",
    "# tokenized_data = tokenizer(all_data, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Prepare input tensors\n",
    "# input_ids = tokenized_data['input_ids']\n",
    "# attention_masks = tokenized_data['attention_mask']\n",
    "\n",
    "# # Create DataLoader\n",
    "# batch_size = 16\n",
    "# train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# # Set device to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Set optimizer and learning rate\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # Fine-tune the model\n",
    "# num_epochs = 3\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(train_dataloader, desc=\"Epoch %d\" % epoch):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2].unsqueeze(1)}  # Unsqueezing labels to match shape\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(**inputs)\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(\"Average training loss:\", total_loss / len(train_dataloader))\n",
    "\n",
    "#     # Test classification\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         keyword = \"scikit-learn\"  # Example keyword to classify\n",
    "#         inputs = tokenizer(keyword, return_tensors='pt').to(device)\n",
    "#         outputs = model(**inputs)\n",
    "#         prediction = torch.sigmoid(outputs.logits).item()\n",
    "#         if prediction > 0.5:\n",
    "#             print(f\"'{keyword}' is classified as a model/library.\")\n",
    "#         else:\n",
    "#             print(f\"'{keyword}' is not classified as a model/library. Prediction: {prediction}\")\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(model.state_dict(), \"keyword_extraction_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn: Model\n",
      "nltk: Model\n",
      "tensorflow: Model\n",
      "pandas: Model\n",
      "gensim: Model\n",
      "domain.com: Model\n",
      "model_name: Library\n",
      "dfgrdrg: Model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import model_names\n",
    "\n",
    "# Load the model names\n",
    "all_data = model_names.model_names\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prompts\n",
    "model_prompt = \"Is '{keyword}' used in machine learning or deep learning?\"\n",
    "library_prompt = \"Is '{keyword}' a Python library commonly used for data analysis or machine learning?\"\n",
    "domain_prompt = \"Does '{keyword}' appear to be a domain name of any field of work?\"\n",
    "\n",
    "# Function to classify a keyword\n",
    "def classify_keyword(keyword):\n",
    "    try:\n",
    "        # Prepare prompts\n",
    "        model_input_ids = tokenizer.encode(model_prompt.format(keyword=keyword), return_tensors=\"pt\")\n",
    "        library_input_ids = tokenizer.encode(library_prompt.format(keyword=keyword), return_tensors=\"pt\")\n",
    "        domain_input_ids = tokenizer.encode(domain_prompt.format(keyword=keyword), return_tensors=\"pt\")\n",
    "\n",
    "        # Generate text completions\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model.generate(model_input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "            library_outputs = model.generate(library_input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "            domain_outputs = model.generate(domain_input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Decode the generated texts\n",
    "        model_generated_text = tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
    "        library_generated_text = tokenizer.decode(library_outputs[0], skip_special_tokens=True)\n",
    "        domain_generated_text = tokenizer.decode(domain_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Check for keywords in generated texts\n",
    "        if \"yes\" in model_generated_text.lower():\n",
    "            return \"Model\"\n",
    "        elif \"yes\" in library_generated_text.lower():\n",
    "            return \"Library\"\n",
    "        elif \"yes\" in domain_generated_text.lower():\n",
    "            return \"Domain\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    except KeyError:\n",
    "        return \"Other\"\n",
    "\n",
    "# Example usage\n",
    "keywords = [\n",
    "    \"scikit-learn\",\n",
    "    \"nltk\",\n",
    "    \"tensorflow\",\n",
    "    \"pandas\",\n",
    "    \"gensim\",\n",
    "    \"domain.com\",\n",
    "    \"model_name\",\n",
    "    \"dfgrdrg\"\n",
    "]\n",
    "\n",
    "for keyword in keywords:\n",
    "    classification = classify_keyword(keyword)\n",
    "    print(f\"{keyword}: {classification}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
